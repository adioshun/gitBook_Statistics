|메인 주제|확률 분포 추정-비 모수적|
|-|-|
|키워드||
|참고자료|오일석3장, |


# 확률 분포 추정

## 2. 비 모수적 방법 
- 창의 크기 고정 유무 

### 2.1 히스토 그램 

히스토그램을 확률 분포로 사용하기 위해서는 각 빈의 값을 N으로 나누어 정규화해 주면 된다. 

단점
- bin의 경계에서 불연속성이 나타난다는 점
- bin의 크기 및 시작 위치에 따라서 히스토그램이 달라진다는 점
- 고차원(high dimension) 데이터에는 메모리 문제 등으로 사용하기 힘들다는 점 등의 문제점


출처: http://darkpgmr.tistory.com/147 [다크 프로그래머]

### 2.2 커널 밀도 추정

- 커널함수(kernel function)를 이용하여 히스토그램 방법의 문제점을 개선한 방법
    
- 대표적 KDE방법 : 파젠 창

#### 가. 히스토그램을 확장하여 확률밀도를 구하는 방법 

###### Step 1. 임의의 점 x에서 확률 값 추정 

크기가 H인 창을 이용하여 확률 구하기 $$ p(x) = \frac{1}{h^d}\frac{k_x}{N}$$
- h : 창크기
- d : 차원
- k : 샘플의 갯수
- N : 전체 샘플의 갯수 

![](http://i.imgur.com/HjTbt2P.png)


###### Step 2. 커널 함수를 이용하여 근접도에 따른 가중치 주기 

x에 가까운 정도에 가중치 주기 (eg. 그림 3.6 a에서 왼쪽 점이 오른쪽 점보다 x에 가까움)   

![](http://i.imgur.com/WO1Co5Z.png?)

계산함수(a)의 커널 함수 : $$k_x = \sum^N_{i=1}k\left(\frac{x-x_i}{h}\right) \rightarrow p(x)=\frac{1}{h^d}\frac{1}{N}\sum^N_{i=1}k\left(\frac{x-x_i}{h}\right)$$

매끄러운(b)의 커널 함수 : 가우시언함수 $$k(x) = \frac{1}{n}\sum^n_{i=1}\frac{1}{(\sqrt[h]{2\pi})^d}exp\left(-\frac{1}{2}\left(\frac{x-x_i}{h}\right)^2\right)$$


###### [참고] 대표적인 커널 함수 
> 커널함수(kernel function): 원점을 중심으로 대칭이면서 적분값이 1인 non-negative 함수 (eg. 가우시언함수)

- Uniform : 계단형태
- Gaussian : 계산의 편의성이 큰 커널 함수(추천 h = $$\left(\frac{4\sigma^5}{3n}\right)^\frac{1}{5}\approx 1.06\sigma n^{-1/5}$$ )
- Epanechnikov  : (위키피디아 추천) 가장 최적의 커널함수 

![](http://i.imgur.com/vUWfKw2.png)


#### 나. 단점 

- 차원의 저주에 자유 롭지 못함, 고차원이 되었을 때의 문제에서는 여전히 자유롭지 못합니다.

- 1차원에서 $$N_1$$개의 샘플이 필요하다면 d차원에서 $$N_1^d$$ 정도가 있어야 믿을만한 확률 분포를 얻으수 있다. 

- 해결 : KNN
    
    https://medium.com/mathpresso/mathpresso-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94-14-%EB%B0%80%EB%8F%84-%EC%B6%94%EC%A0%95-density-estimation-38fd7ef729bb

http://carstart.tistory.com/


### 2.3 K-최근접 이웃 추정 (k-NN Rule)

KDE의 방식 : 고정된 창, 창의 중심 x를 어디에 두느냐에 따라 창 안의 샘플의 개수가 달라진다. 
K-NNR의 방식 : 가변적 창, x를 중심으로 창을 씌우고 k개의 샘플이 창 안에 들어 올때까지 창의 크기를 확장한다. 

### 가. 확률 밀도 구하기 

![](http://i.imgur.com/rGCqf9W.png)
$$
p(x) = \frac{1}{h_x^d}\frac{k}{N}
$$

> KDE와 비교 하여 h는 x에 따라 변하므로 $$h$$를 $$h_x$$로 표기 하였다. 

#### 나. 단점 

창크기 조정을 위한 계산량이 많다. 

해결책 : 보르노이 도형, 훈련집합에 따라 특징 공간을 미리 여러 구간으로 나누어 놓음 

### 다. K-NN분류기 

기본 아이디어는 K-NNR을 활용 : x를 $$\omega_q$$로 분류하라, 이때 $$q=argmax_i k_i$$

$$
K-NN 분류기 = p(\omega_i \mid X) = \frac{P(\omega_i)p(X \mid \omega_i)}{p(X)} = \frac{k_i}{k}
$$

베이즈 공식에 적용 
- $$p(X \mid \omega_i) = \frac{k_i}{k_X^dN_i}$$ : 조건부확률(likelihood)
- $$P(\omega_i)=\frac{N_i}{N}$$ : 사전확률
- $$p(X) = \frac{k}{h_x^dN} $$

###### Step 1. 훈련 샘플 중에 x에 가장 가까운 k개를 찾는다. 

거리 척도 : 유클리디언거리, 마할라노비스 거리 척도 사용 

###### Step 2. k개가 속한 부류를 조사하여 가장 빈도가 높은 류를 $$\omega_q$$라 한다. 



