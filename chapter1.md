# 기본사항  

## 데이터 분석 3대 방법론 
![](/assets/three_main.jpg)

통계란 
관심 대상에 대해 관련된 자료를 수집(`표본추출`, Sampling)하고, 요약(`기술통계`)하고, 불확실한 사실에 대한 결론이나 일반적인 규칙성을 추리(`추론통계`) 하는 학문

### 기술 통계 분석 
* Desccriptive Statistics, 설명(=기술)하기 위한 통계
* 수집된 자료를 정리, 요약하여 수치, 표, 그래포로 자료의 특징을 파악
* eg. 평균 키?

#### 1. 수치로 기술 

#### 2. 표로 기술 

#### 3. 그래프로 기술 


### 추론 통계 분석 
* 표본을 분석하여 모집간에 대해 추측하고 일반화 시키는 연구 분야 
* 새로운 가설이 틀지는지 맞는지 검증 


## 1. 유의성

1. 귀무가설\($$H_0$$, Null Hypothesis, 영가설\) : 아무일도 일어나지 않는다
   * 평균 비교시 두 모집단의 평균이 같다. 
   * 회귀 분석시 그래프의 기울기가 0이다. 
   * 귀무가설은 **반증이 가능해야한다**는 점이 중요하다. 
   * 현재의 가설 
2. 대립가설\($$H_1$$, Alternative Hypothesis\) : 어떤일이 일어난다.

   * 새로운 가설

     > 좋은 가설 = 기각이 가능한 가설=반증이 가능한 가설   
     > by karl Popper

```
연구자가 증명하고자 하는 실험가설과 __반대__되는 입장, 증명되기 전까지는 효과도 없고 차이도 없다는 귀무가설을 세우고, 연구자가 실험을 통해 규명하고자 하는 가설인 대립가설을 세우고 
최종적으로 실험을 통해 귀무가설이 유의수준 5%미만이므로 발생확률이 적어 __기각_하고 대립가설을 __체택__하도록 하는게 목적임 
```

## 2. 해석\(1종 오류 & 2종 오류\)

* 1종 오류\($$ \alpha$$\) : 귀무가설이 참일때 이를 기각한다.\(eg. 차이or효과가 없는데 차이or효과가 있다고 판단\)
* 2종 오류\($$ \beta$$\) : 귀무가설이 거짓일때 이를 받아들인다. \(eg. 차이or효과가 있는데 차이or효과가 없다고 판단\)

> 유의 수준 5%란? 2종 오류보다 1종 오류로 인한 사회적 파장이 큼 - 학계에서 1종 오류를 5% 미만으로 제한\($$p < 0.05$$\)

## 3. _p_값  

* 가설 검증시 검정 통계량 사용 
  * 검정 통계량 = 스튜던트 t, 피셔의 F, 피어슨의 카이제곱
* p값은 검정 통계량의 크기에 대한 개념 
  * eg. 귀무가설이 참이라 가정할 때 관심 사건의 검정 통계량을 계산하고 이 값과 같거나 큰 경우에 놓일 확률의 추정치가 바로 p값
  * 검정 통계량이 크다 = 귀무가설이 참일 것 같지 않음 = 귀무가설 기각하고 대립가설을 받아 들임
* p-value는 귀무가설이 맞다는 전제 하에, 관측된 통계값 혹은 그 값보다 큰 값이 나올 확률이다.[2]  

![](/assets/h_ge.jpg)

## 4. 모형 선택 : 아래의 가정들이 만족하여야 함\(중요도 순\)

1. 무작위 표본 추출
2. 등분산성
3. 오차의 정규성
4. 오차의 독립성
5. 효과의 가산성 

## 5. 최대 가능도\(Maximum likelihood\)

* _**모형이 데이터에 최적으로 적합**_될 때의 모수 값의 특징은 
  * 편이가 없는, 분산을 최소화 하는 추정량 \(Unbiased, variance minimizing estimators\)

## 6. 시험 설계

* 반복 실행 : 신뢰도 높이기 위해
* 무작위화 : 편이\(bias\)를 줄이기 위해

> 올바른 분석을 시행하기 위해서는 아래 주제들도 고려 해야함
>
> * 간결성의 원칙 
> * 검정력\(power\)
> * 통제집단
> * 인위적 반복\(pseudoreplication\)의 확인과 그에 대한 대처
> * 실험적 데이터와 관찰 데이터의 차이점\(비직교성^non-orthogonality\)

### 6.1 간결성의 원칙\(오컴의 면도날\)

* 특정 현상에 대해 비슷한 설명들이 여러 개 있을 경우 가장 단순한 것을 선택 해야 한다. 
  * 14세기 영국 유명론 철학자 윌리엄 오컴
  * 설명을 최소한으로 깍는다\(shave\)는 의미로 면도날이라고 불림
* 통계 모형에서 간결성의 원칙은 다음의 의미를 포함한다. 
  1. 모형은 되도록 가장 작은 수의 모수를 포함해야 한다. 
  2. 비선형 모형보다는 선형 모형을 사용해야 한다. 
  3. 되도록 작은 수의 가정을 고려할 수 있는 실험을 선택 해야 한다.
  4. 모형은 최소 합당한 수준까지 줄여야 한다. 
  5. 복잡한 설명보다는 단순한 설명을 선택해야 한다. 

### 6.2 반복 시행

* 여러 번의 측정이 반복 시행으로 인정되려면 다음의 조건에 부합돼야 한다. 
  1. 독립적이어야 한다. 
  2. 시계열적 개념이 개입되어서는 안된다. 
  3. 공간적인 독립성을 고려해야 한다. 
  4. 적절한 공간 스케일을 사용해 측정이 이뤄져야 한다. 
  5. 이상적으로는 각각의 처치들을 일정 기준의 블록으로 묶어 이를 각각의 시행으로 간주하고 이렇게 만들어진 많은 블록들에서 모든 처치들에 대한 측정이 이루어져야 한다. 
  6. 동일한 개인이나 동일한 공간에서의 반복측정은 반복 시행이 아니다. 

> 가설 검정 시 반복 시행 수는 어떻게 결정 하는가?
>
> * 보통 30번 정도 시행 
> * 결정 방법론은???

### 6.3 검정력\($$ 1-\beta$$\)

* 가설검정에서 귀무가설이 거짓일 때 이를 기각할 확률(기각하여 올바른 결정을 할 수 있는 확률)
  * 계산법 : $$ 1-\beta$$ (2종 오류를 범하지 않을 확률)
  * 차이 or 효과가 있는것을 통계적으로 차이 or 효과가 있다고 보여 주는것 
* 통계기법마다 검정력이 서로 다름 
* 검정력에 영햐을 미치는 요소 
  1. 표본 크기($$n$$) : 다른 조건들이 같다면 포본 크기가 클수록 검정력은 커진다
  2. 유의수준($$\alpha$$) : 유의 수준이 높을수록 검정력이 커진다. 
  3. 검정되는 모수의 __참값__:__참값__과 귀무가설에서 명시된 값의 차이가 클수록 검정력이 커진다. 
> 연구자들은 제 1종 오류를 5%로 유지하면서 검정력을 최대화하는 통계 기법을 사용하고자 함


**중요 시간 가지고 다시**

### 6.4 무작위화

* 선택될 확률이 동일함을 의미
* 
### 7. 직교 설계와 비 직교 설계

* 직교\(orthogonal\) 설계 : Missing 데이터가 없다
* 비직교\(non-orthogonal\) 설계 : Missing 데이터가 있다 --&gt; 비 직교 데이터에서는 순서가 중요 

### 8. Aliasing\(앨리어싱\) :

* 모형의 결과 중 하나 또는 그 이상의 행에서 예사외로 NA가 나타나는 것 
* 모수 추정치에 대한 정보가 없을 때 발생
* 종류 
  * 내적 에일리어싱 : 모형의 구조에 따라 발생
  * 외적 에이리어싱 : 데이터의 속성에 따라 발생 

# 자유도 
* 표본크기 $$n$$에서 데이터로부터 추정하고자 하는 모수 $$p$$의 수를 뺀것
* 평균을 추정할때는 $$n-1$$의 자유도를 가지게 된다. 
  * eg. 5개의 박스에 값을 넣어 평균이 4가 되게 할때, 1~4개의 박스는 아무 값을 넣어도 되지만 마지막 박스는 고정된 값이 된다. 
  * 따라서, 이때의 `자유도는 4`이다.  


### 모집단의 분산 & 표본의 분산   
$$
모집단의 분산 = \sigma^2 = \frac{\sum(y-\overline y)^2}{n}
$$

$$
표본의 분산 = s^2 = \frac{\sum(y-\overline   y)^2}{n-1}
$$
    
> 표본은 모집단이 아니므로 마지막 값을 자유롭게 뽑을수 없음($$n-1$$)
    

# 분산
$$
분산 = s^2 = \frac{제곱합}{자유도} = \frac{\sum(y-\overline y)^2}{n-1}
$$

* 두 데이터의 분산이 같다면(등분산)면 통계 분석(=평균 비교)의 중요한 가정을 만족한것

> 분산이 다르면 평균 비교를 해서는 안된다. 

## 분산의 사용 분야 
#### 1. 비신뢰도(unreliability)의 측정
* 비신뢰도의 측정치 = 표준오차 

$$
비신뢰도 \propto \sqrt{\frac{s^2}{n}}
$$

> [$$\propto$$](1) : 비례

> $$s^2$$: 분산이 커질수록 비신뢰도는 증가 -분자에 위치 

> $$n$$ :표본크기가 커질수록 비신뢰도는 감소 - 분모에 위치

> $$\sqrt{}$$ : 모수의 차원과 같은 차원을 가진 비신뢰도 측정치를 위해 

* 신뢰구간 = 표본 추출이 반목해서 이뤄졌을때 평균이 놓일 수 있는 범위
* 비신뢰도가 크면 신뢰구간도 넓어짐
$$
신뢰구간 \propto  비신뢰도 측정치 \propto \sqrt{\frac{s^2}{n}}
$$

> * 신뢰구간 공식에서 비례($$\propto$$)의 표현을 등호(=) 바꾸려면 `분포`를 적용 하면 된다. 

> * 신뢰구간을 계산하는 또 다른 방법은 `부트스트랩`이 있다.  

#### 2. 가설 검정
* eg. 스튜던트 $$t$$검정 


# 표본 크기(=반복 시행 수)
* 


# 평균 & 표준편차 & 분산
* 데이터는 오차(=치우침)를 관리하고 예측 하는것이 주요 
* 통계 = 오차를 분석하고 관리 하는 학문 
* 치우침을 표현하는 대표적 척도가 `표준편차`(SD[^2])와 `분산`이다.  
* 얼마나 치우쳤는지 비교 하기 위해 __기준점__ 필요 
  * 기준점으로 최빈값, 중앙값, 평균 사용 

> 표준편차 & 분산은 치우침, 평균은 그 기준점 
> * 평균값으로 중심 경향성을 표현하고, 분산으로 얼마나 다양히 퍼져 있는지 표현 

> 표준편차가 있는데 분산을 사용하는 이유 = __음수(-)값으로 인한 상쇄방지__
> $$
표준편차= \sqrt{분산} 
$$

||평균|표준편차|분산|
|-|:-:|:-:|:-:|
|모집단|$$ \mu  $$|$$ \sigma $$|$$ \sigma^2$$|
|표본|$$\overline y $$|$$ S $$|$$ S^2$$|

# 중심경향\(Central tendency\)

* 측정 값들이 어떤 값을 중심으로 모여 있는 양상 

# 중심 경향값\(Measure of contral tendency\)

* 중심 경향값을 나타내는 특정한 값 
* 자료의 대표값, 데이터를 요약하는 한 방법

## 중심 경향값의 종류 \(중심 경향 척도\)

* 산술 평균 \(Mean, $$\overline Y$$\) : 전체 사례의 값을 더한 후 총 사례수로 나눈값

* 가중 평균 \(Weighted mean\)

* 중앙값 \(Median, $$ M_e $$\) " 가장 작은 수 부터 가장 큰 수까지 크기순으로 배열했을때 중앙에 위치하는 사례의 값

* 최빈값 \(Mode, ??\) : 가장 많은 도수를 같는 점수

> 중심 경향값으로 살펴본 분포
>
> * 정규 분포 평균, 중앙값, 최빈값이 일치하며 좌우 대칭이 되는 평태의 분포

| 평균 | 표현 | 계산법 | 설명 | 특징 | R함수 |
| --- | --- | --- | --- | --- | --- |
| 산술평균 | $$ \overline{y} $$ \(bar\) | $$ \overline{y} = \frac{\sum y}{n} $$ | 데이터를 모두 더한\($$\sum y$$\) 다음 데이터 수\($$n$$\)로 나눔 | 이상치 영향을 받음->중앙값으로 해결 | mean\(\) |
| 기하평균 | $$ \hat{y} $$ \(hat\) | $$\hat{y} = \sqrt[n]{\prod y}$$ | 데이터를 모든 곱한 값의 $$n$$제곱근 | 로그\(Logarithms\)를 이용해 구할수도 있음 |  |
| 조화평균 | $$ \tilde{y} $$ | $$\tilde{y} = \frac{1}{\frac{\sum\frac{1}{y}}{n}} = \frac{n}{\sum\frac{1}{y}}$$ |  |  |  |

> $$\sum y$$ : 모든것을 더하다\(SIGMA\)
>
> $$\prod y$$ : 모든것을 곱한다\(pi\)
>
> $$ \tilde{y} $$ : \(curl\)

* 곱의 형식으로 변화 하는 데이터를 다룰때는 원래의 데이터에 로그를 적용해 그래프를 그려 봐야 한다.  
* 로그의 영향을 제거하기 위해 antilog\(exp\)를 적용한다. 
* 기하 평균 = exp\(mean\(log\(데이터\)\)\) =  데이터에 로그를 적용하고 산술평균을 구한후 이에 대한 antilog를 계산 

## 산술 평균 (AM, $$ \overline{y} $$ )
* 두 수를 더해서 둘로 나누는것 
* 두 수는 물리량을 의미 해야 한다. 
    * 물리량은 무게, 길이 등 양적으로 측정되어 사칙 연산이 가능해야함

> * 산술평균으로 `배수/비율`값에 대한 계산은다 맞지 않는다. 


## [기하 평균][1] (GM)
* 배수/비율에 대한 평균 계산 방식 


## [조화 평균][2] (HM)

# 중심 극한 정리\(Central limit theorem\)

* \(데이터 자체는 어떤 중심 값 주변으로 모이는 경향이 없더라도\) 반복적 실험에 의해 **계산된 값**들은 중김 값 주변에 모이는 경향이 있다. 
* 표본의 크기가 충분히 크다면 표본 변수들의 그 합 또는 평균의 히스토 그램은 **정규분포** 곡선에 수렴한다. 

> 많은 통계 모형들은 자료가 정규 분포라는 가정에 기초 하여 시행한다. 데이터가 충분히 크다면 **중심 극한 정리**에 의하여 정규분포라고 가정하고 통계 분석을 할 수 있다.







# Correlation coefficients 
### Continuous vs. Continuous
* cor(x, use, method = “Pearson OR spearman OR kendall”)
  * Pearson :  quantitative variables 간의 degree of linear relationship 
  * Spearman’s :  rank-ordered variables. 간의 degree of relationship 
  * Kendall’s Tau : nonparametric measure of rank correlation
* cor.test() : 

### Continuous vs. Nominal
* Significance tests: run an ANOVA. In R, you can use ?aov.
* Effect size (strength of association): intraclass correlation, ?icc

### Nominal vs. Nominal
* Significance tests: run a chi-squared test. In R, you use ?chisq.test.
* Effect size (strength of association): calculate Cramer's V. In R, you can use ?assocstats in the vcd package.
* 명목형 && 이분변수 : 크래머(Cramer’s phi)

http://stats.stackexchange.com/questions/108007/correlations-with-categorical-variables
http://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618 


--- 



---

## ---

*[AM]: Arithmetic Mean

*[GM]: Geometric Mean

*[HM]: Harmonic Mean

[^1]: 테스트 

[1]: http://blog.naver.com/bongshinkim/220624355010
[2]: http://iphone_dev.blog.me/80146940161


[1]:http://math7.tistory.com/21


---
[1] : https://en.wiktionary.org/wiki/Appendix:Unicode/Mathematical_Operators "Appendix:Unicode/Mathematical Operators"
[2]: http://adnoctum.tistory.com/332

[^2]: SD = Standard Deviation 